---
layout: post
categories:
title: Nonlinear inverse problems in audio
subtitle: A fun little demonstration
author: Zev Woodstock
use_math: true
tags: []
featured-image: /media/sound.png
date: 2022-03-30
---

Alright, so you're out one evening and you see a fantastic
musician. The live performance sounds clean -- like this:

{% include embed-audio.html src="/media/Original.wav" %}

You wish that you could hear those sweet, sweet tunes again.
However, you are only armed with a mediocre cell phone recording...

{% include embed-audio.html src="/media/Clipped.wav" %}

... and the author's "studio" version, where they added a bunch of
effects muddying the original tones.

{% include embed-audio.html src="/media/Distorted-Echo-Frequency.wav" %}

Is it possible to recover or approximate the original audio tune?

Yes, using the nonlinear recovery framework I developed with
Patrick L. Combettes:

{% include embed-audio.html src="/media/Recovered.wav" %}

How do we do this? Let's start with a brief detour into audio
processing.

<h1> Modeling the distortion of sound </h1>

The WAV file format typically samples a sound $44,100$ times per
second, yielding amplitudes in $[-1,1]$. Since we're working
with a sample which is just over $7$ seconds, we're living in
$\mathbb{R}^{N}$ where $N>308,700$. 

<h2>Bad recordings </h2> 
The phenomenon of "clipping" is often modeled via a truncation of
amplitude. This is what happens when we clip all of the amplitudes
of the original sound $\overline{x}$ to $[-0.05,0.05]$:

{% include embed-audio.html src="/media/Clipped.wav" %}

Mathematically, we can model this observation process via the
projection onto the hypercube $[-0.05,0.05]^N$, i.e.,
$T_1=\textrm{proj}_{[-0.05,0.05]^N}$. Note that $F_1$ is nonlinear
and nonsmooth. 

<div class="image">
<center>
      <img src="/media/clip.png" style="width:85%"><br>
<em><b>Figure 1:</b> An example of hard clipping. Left to right:
Original signal; Characteristic curve of hard-clipping; clipped
version of the original signal.</em>
</center>
</div><br>


<h2> Modeling echo, reverb, and frequency modulation</h2>

Next we are going to mathematically model the operation which takes
the original sound and outputs this "artistic" sample:

{% include embed-audio.html src="/media/Distorted-Echo-Frequency.wav" %}

We can hear a few things from this sample:
<ul>
<li> Distortion has been added.</li>
<li> Echo has been added </li>
<li> 
It sounds like it has gone through an analogue telephone. Instead
of having a full frequency bandwidth, only a subset of frequencies
are transmitted, i.e., the signal was <em>bandlimited</em>.
</li>
</ul>

Note: for further details on these models, check out <em>Hack
Audio</em> by Eric Tarr.

We can estimate the time duration of the echo (roughly $0.3$
seconds), and model this process using discrete convolution, which
I'll represent with a linear operator $L_{1}$ . Likewise, using a
little bit of Fourier analysis, we can encode the process of
bandlimit to the frequency range of an analogue telephone
($400$ -- $3400$ Hz) via a linear operator $L_{2}$.

Finally, there are two types of nonlinear distortion in this clip
-- half-wave rectification $F_1$ and soft-clipping arctangent
distortion $F_2$. Altogether, we have a superposition of waveforms
with all of these effects, shown in the circuit diagram in Figure
2.

<div class="image">
<center>
<img src="/media/dist.png" style="width:60%"><br>
<em><b>Figure 2:</b> Circuit diagram showing the distortion
model.</em>
</center>
</div><br>

Concretely, this process is modeled by $T_2 = L_1^\top\circ F_1 \circ
L_1 + L_2^\top \circ F_2 + L_2$.

<h2>Defining the inverse problem</h2>
Now that we have our operators, we seek to

$$\text{find}\;\;x\in [-1,1]^N\;\;\text{such
that}\;\;T_1x=p_1\;\;\text{and}\;\;T_2x=p_2,
\tag{$*$}
$$

where $p_1=T_1\overline{x}$ and $p_2=T_2\overline{x}$ respectively
denote our clipped and distorted observations.

<h1>Here's where the math comes in</h1>

At first, one may be tempted to solve the following 

$$
\textrm{minimize}_{x\in[-1,1]^N}
\|T_1x-p_1\|^2+\|T_2x-p_2\|^2.
$$

However, since $T_1$ and $T_2$ are nonlinear operators, we run into
some numerical and theoretical issues since the objective function
is nonconvex. In general, this miminimization problem is solved
iteratively with algorithms which only yield local convergence to a
local minimizer.

Nonetheless, there are algorithms which can solve $(*)$ and are
proven to converge to a solution from any initial iterate. These
algorithms are tractable since they rely on
evaluating the forward-operators $T_1$ and $T_2$. It turns out that
both $T_1$ and $T_2$ can be described by firmly nonexpansive
operators, which exactly fits the nonlinear model described <a
href="/publications/2021-08-01/jat.html">here</a>.
(see also <a
href="/publications/2020-12-14/fixedpoint.html">
here</a> for an extrapolated feasibility algorithm and
or <a href="/publications/2022-01-24/icassp2.html">
here</a> for an extension incase the operators are poorly modeled).

Anyhow, using the modeling and results from this series of papers
gives you the following recovery. Thanks for reading, and rock on!

{% include embed-audio.html src="/media/Recovered.wav" %}


A few implementation notes and random comments:
<ul>
<li>
If there are no solutions to the inverse problem $(*)$ (e.g., we
got the frequency range incorrect, or we incorrectly estimated the
echo delay), I would recommend using the algorithm from <a
href="/publications/2022-01-15/siam.html">this paper</a>.
It is proven to converge under very mild conditions, even if $(*)$
has no solutions. The point we converge to solves a relaxation of
$(*)$. For this specific audio problem, since the hypercube
$[-1,1]^N$ is bounded, the algorithm in the paper above is
guaranteed to converge.
</li>
<li>
To handle the hard constraint $x\in[-1,1]^N$, we have a few options
-- one could just rely on the projector which is easily computed.
However, if for your problem it's expensive (or impossible) to
compute the projection, you can also use a subgradient projector,
which is sortof like a subgradient descent-step. These are quite
useful if you have hard constraints described by convex lower
semicontinuous inequalities, as was done in <a
href="/media/publications/eusipco2020.pdf">this
article</a>. 
</li>
<li>
The convergence proof for these algorithms relies heavily on the
fact that these nonlinearities are <em>firmly nonexpansive</em>.
This is a rather broad class of nonlinear operators which we can
use to mathematically model a variety of nonlinearities cropping up
in the wild. In fact, even if your operator fails to be firmly
nonexpansive, you may be able to solve your problem using the
notion of <a
href="/publications/2022-01-15/siam.html"><em>proxification</em></a>.
In short, sometimes by "gluing" discontinuities, rescaling, and
applying the occasional adjoint, one can artificially create a
firmly nonexpansive operator which equivalently encodes your
nonlinear equation. 
</li>
</ul>


